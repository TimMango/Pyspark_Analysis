{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframe Manipulation and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init('/Applications/spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "\n",
    "import pyspark # only run this after findspark.init()\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize the Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to begin with initilize the Spark Session. DataFrame can be created and registered as tables. Moreover, SQL tables be executed, tables can be cached, and parquet/json/csv/avro data formatted files can be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName(\"PysparkExample\").config (\"spark.sql.shuffle.partitions\", \"50\").config(\"spark.driver.maxResultSize\",\"5g\").config (\"spark.sql.execution.arrow.enabled\", \"true\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.145:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PysparkExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f85f81b74d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the Kaggle dataset, which includes the book title, author, the date of the best seller list, the published date of the list, the book description, the rank (this week and last week), the publisher, number of weeks on the list, and the price [Link](https://www.kaggle.com/cmenca/new-york-times-hardcover-fiction-best-sellers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is so Awesome that it supports all different types of data to be read.\n",
    "\n",
    "DataFrames can be created by reading txt, csv, json and parquet file formats. In our example, we will be using .json formatted file. You can also find and read text, csv and parquet file formats by using the related read functions as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON\n",
    "df = sc.read.json('./data/nyt2.json')\n",
    "\n",
    "#TXT files\n",
    "# dataframe_txt = sc.read.text('./data/text_data.txt')\n",
    "\n",
    "#CSV files\n",
    "# dataframe_csv = sc.read.csv('./data/csv_data.csv')\n",
    "\n",
    "#PARQUET files\n",
    "# dataframe_parquet = sc.read.load('./data/parquet_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the data with show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: StructType(List(StructField($oid,StringType,true)))\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>amazon_product_url</th>\n",
       "      <th>author</th>\n",
       "      <th>bestsellers_date</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>published_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>rank</th>\n",
       "      <th>rank_last_week</th>\n",
       "      <th>title</th>\n",
       "      <th>weeks_on_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(5b4aa4ead3089013507db18b,)</td>\n",
       "      <td>http://www.amazon.com/Odd-Hours-Dean-Koontz/dp...</td>\n",
       "      <td>Dean R Koontz</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>Odd Thomas, who can communicate with the dead,...</td>\n",
       "      <td>(None, 27)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Bantam</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>ODD HOURS</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(5b4aa4ead3089013507db18c,)</td>\n",
       "      <td>http://www.amazon.com/The-Host-Novel-Stephenie...</td>\n",
       "      <td>Stephenie Meyer</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>Aliens have taken control of the minds and bod...</td>\n",
       "      <td>(25.99, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Little, Brown</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(1,)</td>\n",
       "      <td>THE HOST</td>\n",
       "      <td>(3,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(5b4aa4ead3089013507db18d,)</td>\n",
       "      <td>http://www.amazon.com/Love-Youre-With-Emily-Gi...</td>\n",
       "      <td>Emily Giffin</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>A woman's happy marriage is shaken when she en...</td>\n",
       "      <td>(24.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>St. Martin's</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>LOVE THE ONE YOU'RE WITH</td>\n",
       "      <td>(2,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(5b4aa4ead3089013507db18e,)</td>\n",
       "      <td>http://www.amazon.com/The-Front-Garano-Patrici...</td>\n",
       "      <td>Patricia Cornwell</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>A Massachusetts state investigator and his tea...</td>\n",
       "      <td>(22.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Putnam</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>THE FRONT</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(5b4aa4ead3089013507db18f,)</td>\n",
       "      <td>http://www.amazon.com/Snuff-Chuck-Palahniuk/dp...</td>\n",
       "      <td>Chuck Palahniuk</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>An aging porn queens aims to cap her career by...</td>\n",
       "      <td>(24.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>(0,)</td>\n",
       "      <td>SNUFF</td>\n",
       "      <td>(1,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(5b4aa4ead3089013507db190,)</td>\n",
       "      <td>http://www.amazon.com/Sundays-at-Tiffanys-Jame...</td>\n",
       "      <td>James Patterson and Gabrielle Charbonnet</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>A woman finds an unexpected love</td>\n",
       "      <td>(24.99, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Little, Brown</td>\n",
       "      <td>(6,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>SUNDAYS AT TIFFANYâ€™S</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(5b4aa4ead3089013507db191,)</td>\n",
       "      <td>http://www.amazon.com/Phantom-Prey-John-Sandfo...</td>\n",
       "      <td>John Sandford</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>The Minneapolis detective Lucas Davenport inve...</td>\n",
       "      <td>(26.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Putnam</td>\n",
       "      <td>(7,)</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>PHANTOM PREY</td>\n",
       "      <td>(3,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(5b4aa4ead3089013507db192,)</td>\n",
       "      <td>http://www.amazon.com/From-Worse-Southern-Vamp...</td>\n",
       "      <td>Jimmy Buffett</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>A Southern family tries to hide its pet pig at...</td>\n",
       "      <td>(21.99, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Little, Brown</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>(6,)</td>\n",
       "      <td>SWINE NOT?</td>\n",
       "      <td>(2,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(5b4aa4ead3089013507db193,)</td>\n",
       "      <td>http://www.amazon.com/Where-Are-You-Now-Novel/...</td>\n",
       "      <td>Elizabeth George</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>In Cornwall, trying to recover from his wife's...</td>\n",
       "      <td>(27.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Harper</td>\n",
       "      <td>(9,)</td>\n",
       "      <td>(8,)</td>\n",
       "      <td>CARELESS IN RED</td>\n",
       "      <td>(3,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(5b4aa4ead3089013507db194,)</td>\n",
       "      <td>http://www.amazon.com/The-Whole-Truth-David-Ba...</td>\n",
       "      <td>David Baldacci</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>An intelligence agent and a journalist team up...</td>\n",
       "      <td>(26.99, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Grand Central</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>(7,)</td>\n",
       "      <td>THE WHOLE TRUTH</td>\n",
       "      <td>(5,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0  (5b4aa4ead3089013507db18b,)   \n",
       "1  (5b4aa4ead3089013507db18c,)   \n",
       "2  (5b4aa4ead3089013507db18d,)   \n",
       "3  (5b4aa4ead3089013507db18e,)   \n",
       "4  (5b4aa4ead3089013507db18f,)   \n",
       "5  (5b4aa4ead3089013507db190,)   \n",
       "6  (5b4aa4ead3089013507db191,)   \n",
       "7  (5b4aa4ead3089013507db192,)   \n",
       "8  (5b4aa4ead3089013507db193,)   \n",
       "9  (5b4aa4ead3089013507db194,)   \n",
       "\n",
       "                                  amazon_product_url  \\\n",
       "0  http://www.amazon.com/Odd-Hours-Dean-Koontz/dp...   \n",
       "1  http://www.amazon.com/The-Host-Novel-Stephenie...   \n",
       "2  http://www.amazon.com/Love-Youre-With-Emily-Gi...   \n",
       "3  http://www.amazon.com/The-Front-Garano-Patrici...   \n",
       "4  http://www.amazon.com/Snuff-Chuck-Palahniuk/dp...   \n",
       "5  http://www.amazon.com/Sundays-at-Tiffanys-Jame...   \n",
       "6  http://www.amazon.com/Phantom-Prey-John-Sandfo...   \n",
       "7  http://www.amazon.com/From-Worse-Southern-Vamp...   \n",
       "8  http://www.amazon.com/Where-Are-You-Now-Novel/...   \n",
       "9  http://www.amazon.com/The-Whole-Truth-David-Ba...   \n",
       "\n",
       "                                     author     bestsellers_date  \\\n",
       "0                             Dean R Koontz  ((1211587200000,),)   \n",
       "1                           Stephenie Meyer  ((1211587200000,),)   \n",
       "2                              Emily Giffin  ((1211587200000,),)   \n",
       "3                         Patricia Cornwell  ((1211587200000,),)   \n",
       "4                           Chuck Palahniuk  ((1211587200000,),)   \n",
       "5  James Patterson and Gabrielle Charbonnet  ((1211587200000,),)   \n",
       "6                             John Sandford  ((1211587200000,),)   \n",
       "7                             Jimmy Buffett  ((1211587200000,),)   \n",
       "8                          Elizabeth George  ((1211587200000,),)   \n",
       "9                            David Baldacci  ((1211587200000,),)   \n",
       "\n",
       "                                         description          price  \\\n",
       "0  Odd Thomas, who can communicate with the dead,...     (None, 27)   \n",
       "1  Aliens have taken control of the minds and bod...  (25.99, None)   \n",
       "2  A woman's happy marriage is shaken when she en...  (24.95, None)   \n",
       "3  A Massachusetts state investigator and his tea...  (22.95, None)   \n",
       "4  An aging porn queens aims to cap her career by...  (24.95, None)   \n",
       "5                   A woman finds an unexpected love  (24.99, None)   \n",
       "6  The Minneapolis detective Lucas Davenport inve...  (26.95, None)   \n",
       "7  A Southern family tries to hide its pet pig at...  (21.99, None)   \n",
       "8  In Cornwall, trying to recover from his wife's...  (27.95, None)   \n",
       "9  An intelligence agent and a journalist team up...  (26.99, None)   \n",
       "\n",
       "        published_date      publisher   rank rank_last_week  \\\n",
       "0  ((1212883200000,),)         Bantam   (1,)           (0,)   \n",
       "1  ((1212883200000,),)  Little, Brown   (2,)           (1,)   \n",
       "2  ((1212883200000,),)   St. Martin's   (3,)           (2,)   \n",
       "3  ((1212883200000,),)         Putnam   (4,)           (0,)   \n",
       "4  ((1212883200000,),)      Doubleday   (5,)           (0,)   \n",
       "5  ((1212883200000,),)  Little, Brown   (6,)           (3,)   \n",
       "6  ((1212883200000,),)         Putnam   (7,)           (4,)   \n",
       "7  ((1212883200000,),)  Little, Brown   (8,)           (6,)   \n",
       "8  ((1212883200000,),)         Harper   (9,)           (8,)   \n",
       "9  ((1212883200000,),)  Grand Central  (10,)           (7,)   \n",
       "\n",
       "                      title weeks_on_list  \n",
       "0                 ODD HOURS          (1,)  \n",
       "1                  THE HOST          (3,)  \n",
       "2  LOVE THE ONE YOU'RE WITH          (2,)  \n",
       "3                 THE FRONT          (1,)  \n",
       "4                     SNUFF          (1,)  \n",
       "5      SUNDAYS AT TIFFANYâ€™S          (4,)  \n",
       "6              PHANTOM PREY          (3,)  \n",
       "7                SWINE NOT?          (2,)  \n",
       "8           CARELESS IN RED          (3,)  \n",
       "9           THE WHOLE TRUTH          (5,)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show top of dataframe in pandas\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Data Inspection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually want to simply skim through the dataframe before going deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'struct<$oid:string>'),\n",
       " ('amazon_product_url', 'string'),\n",
       " ('author', 'string'),\n",
       " ('bestsellers_date', 'struct<$date:struct<$numberLong:string>>'),\n",
       " ('description', 'string'),\n",
       " ('price', 'struct<$numberDouble:string,$numberInt:string>'),\n",
       " ('published_date', 'struct<$date:struct<$numberLong:string>>'),\n",
       " ('publisher', 'string'),\n",
       " ('rank', 'struct<$numberInt:string>'),\n",
       " ('rank_last_week', 'struct<$numberInt:string>'),\n",
       " ('title', 'string'),\n",
       " ('weeks_on_list', 'struct<$numberInt:string>')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------+--------------------+---------+------------------+\n",
      "|summary|  amazon_product_url|         author|         description|publisher|             title|\n",
      "+-------+--------------------+---------------+--------------------+---------+------------------+\n",
      "|  count|               10195|          10195|               10195|    10195|             10195|\n",
      "|   mean|                null|           null|                null|     null|1877.7142857142858|\n",
      "| stddev|                null|           null|                null|     null| 370.9760613506458|\n",
      "|    min|http://www.amazon...|        AJ Finn|                    |      ACE|  10TH ANNIVERSARY|\n",
      "|    max|https://www.amazo...|various authors|â€™Tis for the Rebe...|allantine|               ZOO|\n",
      "+-------+--------------------+---------------+--------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computes summary statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts the number of rows in dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10195"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts the number of distinct rows in dataframe\n",
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation[_id#6,amazon_product_url#7,author#8,bestsellers_date#9,description#10,price#11,published_date#12,publisher#13,rank#14,rank_last_week#15,title#16,weeks_on_list#17] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "_id: struct<$oid:string>, amazon_product_url: string, author: string, bestsellers_date: struct<$date:struct<$numberLong:string>>, description: string, price: struct<$numberDouble:string,$numberInt:string>, published_date: struct<$date:struct<$numberLong:string>>, publisher: string, rank: struct<$numberInt:string>, rank_last_week: struct<$numberInt:string>, title: string, weeks_on_list: struct<$numberInt:string>\n",
      "Relation[_id#6,amazon_product_url#7,author#8,bestsellers_date#9,description#10,price#11,published_date#12,publisher#13,rank#14,rank_last_week#15,title#16,weeks_on_list#17] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation[_id#6,amazon_product_url#7,author#8,bestsellers_date#9,description#10,price#11,published_date#12,publisher#13,rank#14,rank_last_week#15,title#16,weeks_on_list#17] json\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) FileScan json [_id#6,amazon_product_url#7,author#8,bestsellers_date#9,description#10,price#11,published_date#12,publisher#13,rank#14,rank_last_week#15,title#16,weeks_on_list#17] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/Mango/Desktop/Spark_Pipeline/Spark_Analysis_Examples/data/nyt2.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_id:struct<$oid:string>,amazon_product_url:string,author:string,bestsellers_date:struct<$d...\n"
     ]
    }
   ],
   "source": [
    "# Prints plans including physical and logical\n",
    "df.explain(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Useful common functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] Remove Duplicate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate values in a table can be eliminated by using dropDuplicates() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There were no duplicates\n",
    "df_dropdup = df.dropDuplicates() \n",
    "df_dropdup.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] 'Select' Operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select creates a new dataframe using criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to obtain columns by column or by indexing (i.e. dataframe[â€˜authorâ€™])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              author|\n",
      "+--------------------+\n",
      "|       Dean R Koontz|\n",
      "|     Stephenie Meyer|\n",
      "|        Emily Giffin|\n",
      "|   Patricia Cornwell|\n",
      "|     Chuck Palahniuk|\n",
      "|James Patterson a...|\n",
      "|       John Sandford|\n",
      "|       Jimmy Buffett|\n",
      "|    Elizabeth George|\n",
      "|      David Baldacci|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------------------+----+--------+\n",
      "|              author|               title|rank|   price|\n",
      "+--------------------+--------------------+----+--------+\n",
      "|       Dean R Koontz|           ODD HOURS| [1]|  [, 27]|\n",
      "|     Stephenie Meyer|            THE HOST| [2]|[25.99,]|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...| [3]|[24.95,]|\n",
      "|   Patricia Cornwell|           THE FRONT| [4]|[22.95,]|\n",
      "|     Chuck Palahniuk|               SNUFF| [5]|[24.95,]|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANYâ€™S| [6]|[24.99,]|\n",
      "|       John Sandford|        PHANTOM PREY| [7]|[26.95,]|\n",
      "|       Jimmy Buffett|          SWINE NOT?| [8]|[21.99,]|\n",
      "|    Elizabeth George|     CARELESS IN RED| [9]|[27.95,]|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|[10]|[26.99,]|\n",
      "+--------------------+--------------------+----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show all entries in title column\n",
    "df.select(\"author\").show(10)\n",
    "\n",
    "#Show all entries in title, author, rank, price columns\n",
    "df.select(\"author\", \"title\", \"rank\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   price|\n",
      "+--------+\n",
      "|  [, 27]|\n",
      "|[25.99,]|\n",
      "|[24.95,]|\n",
      "|[22.95,]|\n",
      "|[24.95,]|\n",
      "|[24.99,]|\n",
      "|[26.95,]|\n",
      "|[21.99,]|\n",
      "|[27.95,]|\n",
      "|[26.99,]|\n",
      "|  [, 27]|\n",
      "|[26.95,]|\n",
      "|[23.95,]|\n",
      "|[24.95,]|\n",
      "|  [, 28]|\n",
      "|   [, 0]|\n",
      "|   [, 0]|\n",
      "|   [, 0]|\n",
      "|   [, 0]|\n",
      "|   [, 0]|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe of several columns\n",
    "cases = cases.select('province','city','infection_case','confirmed')\n",
    "cases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by highest to lowest value of a column\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "cases.sort(F.desc(\"confirmed\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3] 'When' Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3f5071032c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show title and assign 0 or 1 depending on title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ODD HOURS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# Show title and assign 0 or 1 depending on title\n",
    "df.select(\"title\", when(dataframe.title != 'ODD HOURS', 1).otherwise(0)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4] 'isin' Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: StructType(List(StructField($oid,StringType,true)))\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>amazon_product_url</th>\n",
       "      <th>author</th>\n",
       "      <th>bestsellers_date</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>published_date</th>\n",
       "      <th>publisher</th>\n",
       "      <th>rank</th>\n",
       "      <th>rank_last_week</th>\n",
       "      <th>title</th>\n",
       "      <th>weeks_on_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(5b4aa4ead3089013507db18d,)</td>\n",
       "      <td>http://www.amazon.com/Love-Youre-With-Emily-Gi...</td>\n",
       "      <td>Emily Giffin</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>A woman's happy marriage is shaken when she en...</td>\n",
       "      <td>(24.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>St. Martin's</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>LOVE THE ONE YOU'RE WITH</td>\n",
       "      <td>(2,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(5b4aa4ead3089013507db191,)</td>\n",
       "      <td>http://www.amazon.com/Phantom-Prey-John-Sandfo...</td>\n",
       "      <td>John Sandford</td>\n",
       "      <td>((1211587200000,),)</td>\n",
       "      <td>The Minneapolis detective Lucas Davenport inve...</td>\n",
       "      <td>(26.95, None)</td>\n",
       "      <td>((1212883200000,),)</td>\n",
       "      <td>Putnam</td>\n",
       "      <td>(7,)</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>PHANTOM PREY</td>\n",
       "      <td>(3,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(5b4aa4ead3089013507db1a2,)</td>\n",
       "      <td>http://www.amazon.com/Love-Youre-With-Emily-Gi...</td>\n",
       "      <td>Emily Giffin</td>\n",
       "      <td>((1212192000000,),)</td>\n",
       "      <td>A womanâ€™s happy marriage is shaken when she en...</td>\n",
       "      <td>(24.95, None)</td>\n",
       "      <td>((1213488000000,),)</td>\n",
       "      <td>St. Martinâ€™s</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>LOVE THE ONE YOU'RE WITH</td>\n",
       "      <td>(3,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(5b4aa4ead3089013507db1a7,)</td>\n",
       "      <td>http://www.amazon.com/Phantom-Prey-John-Sandfo...</td>\n",
       "      <td>John Sandford</td>\n",
       "      <td>((1212192000000,),)</td>\n",
       "      <td>The Minneapolis detective Lucas Davenport inve...</td>\n",
       "      <td>(26.95, None)</td>\n",
       "      <td>((1213488000000,),)</td>\n",
       "      <td>Putnam</td>\n",
       "      <td>(9,)</td>\n",
       "      <td>(7,)</td>\n",
       "      <td>PHANTOM PREY</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(5b4aa4ead3089013507db1b6,)</td>\n",
       "      <td>http://www.amazon.com/Love-Youre-With-Emily-Gi...</td>\n",
       "      <td>Emily Giffin</td>\n",
       "      <td>((1212796800000,),)</td>\n",
       "      <td>A womanâ€™s happy marriage is shaken when she en...</td>\n",
       "      <td>(24.95, None)</td>\n",
       "      <td>((1214092800000,),)</td>\n",
       "      <td>St. Martinâ€™s</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>(4,)</td>\n",
       "      <td>LOVE THE ONE YOU'RE WITH</td>\n",
       "      <td>(4,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  \\\n",
       "0  (5b4aa4ead3089013507db18d,)   \n",
       "1  (5b4aa4ead3089013507db191,)   \n",
       "2  (5b4aa4ead3089013507db1a2,)   \n",
       "3  (5b4aa4ead3089013507db1a7,)   \n",
       "4  (5b4aa4ead3089013507db1b6,)   \n",
       "\n",
       "                                  amazon_product_url         author  \\\n",
       "0  http://www.amazon.com/Love-Youre-With-Emily-Gi...   Emily Giffin   \n",
       "1  http://www.amazon.com/Phantom-Prey-John-Sandfo...  John Sandford   \n",
       "2  http://www.amazon.com/Love-Youre-With-Emily-Gi...   Emily Giffin   \n",
       "3  http://www.amazon.com/Phantom-Prey-John-Sandfo...  John Sandford   \n",
       "4  http://www.amazon.com/Love-Youre-With-Emily-Gi...   Emily Giffin   \n",
       "\n",
       "      bestsellers_date                                        description  \\\n",
       "0  ((1211587200000,),)  A woman's happy marriage is shaken when she en...   \n",
       "1  ((1211587200000,),)  The Minneapolis detective Lucas Davenport inve...   \n",
       "2  ((1212192000000,),)  A womanâ€™s happy marriage is shaken when she en...   \n",
       "3  ((1212192000000,),)  The Minneapolis detective Lucas Davenport inve...   \n",
       "4  ((1212796800000,),)  A womanâ€™s happy marriage is shaken when she en...   \n",
       "\n",
       "           price       published_date     publisher  rank rank_last_week  \\\n",
       "0  (24.95, None)  ((1212883200000,),)  St. Martin's  (3,)           (2,)   \n",
       "1  (26.95, None)  ((1212883200000,),)        Putnam  (7,)           (4,)   \n",
       "2  (24.95, None)  ((1213488000000,),)  St. Martinâ€™s  (4,)           (3,)   \n",
       "3  (26.95, None)  ((1213488000000,),)        Putnam  (9,)           (7,)   \n",
       "4  (24.95, None)  ((1214092800000,),)  St. Martinâ€™s  (4,)           (4,)   \n",
       "\n",
       "                      title weeks_on_list  \n",
       "0  LOVE THE ONE YOU'RE WITH          (2,)  \n",
       "1              PHANTOM PREY          (3,)  \n",
       "2  LOVE THE ONE YOU'RE WITH          (3,)  \n",
       "3              PHANTOM PREY          (4,)  \n",
       "4  LOVE THE ONE YOU'RE WITH          (4,)  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show rows with specified authors if in the given options\n",
    "df[df.author.isin(\"John Sandford\", \"Emily Giffin\")].toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5] 'Like' Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|              author|               title|title LIKE % THE %|\n",
      "+--------------------+--------------------+------------------+\n",
      "|       Dean R Koontz|           ODD HOURS|             false|\n",
      "|     Stephenie Meyer|            THE HOST|             false|\n",
      "|        Emily Giffin|LOVE THE ONE YOU'...|              true|\n",
      "|   Patricia Cornwell|           THE FRONT|             false|\n",
      "|     Chuck Palahniuk|               SNUFF|             false|\n",
      "|James Patterson a...|SUNDAYS AT TIFFANYâ€™S|             false|\n",
      "|       John Sandford|        PHANTOM PREY|             false|\n",
      "|       Jimmy Buffett|          SWINE NOT?|             false|\n",
      "|    Elizabeth George|     CARELESS IN RED|             false|\n",
      "|      David Baldacci|     THE WHOLE TRUTH|             false|\n",
      "|        Troy Denning|          INVINCIBLE|             false|\n",
      "|          James Frey|BRIGHT SHINY MORNING|             false|\n",
      "|         Garth Stein|THE ART OF RACING...|              true|\n",
      "|     Debbie Macomber|       TWENTY WISHES|             false|\n",
      "|         Jeff Shaara|      THE STEEL WAVE|             false|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show author and title is TRUE if title has \" THE \" word in titles\n",
    "df.select(\"author\", \"title\", df.title.like(\"% THE %\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6] 'Startswith' â€” 'Endswith' Operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StartsWith scans from the beginning of word/content with specified criteria in the brackets. In parallel, EndsWith processes the word/content starting from the end. Both of the functions are case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------------------+\n",
      "|           author|               title|startswith(title, THE)|\n",
      "+-----------------+--------------------+----------------------+\n",
      "|    Dean R Koontz|           ODD HOURS|                 false|\n",
      "|  Stephenie Meyer|            THE HOST|                  true|\n",
      "|     Emily Giffin|LOVE THE ONE YOU'...|                 false|\n",
      "|Patricia Cornwell|           THE FRONT|                  true|\n",
      "|  Chuck Palahniuk|               SNUFF|                 false|\n",
      "+-----------------+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+--------------------+-------------------+\n",
      "|           author|               title|endswith(title, NT)|\n",
      "+-----------------+--------------------+-------------------+\n",
      "|    Dean R Koontz|           ODD HOURS|              false|\n",
      "|  Stephenie Meyer|            THE HOST|              false|\n",
      "|     Emily Giffin|LOVE THE ONE YOU'...|              false|\n",
      "|Patricia Cornwell|           THE FRONT|               true|\n",
      "|  Chuck Palahniuk|               SNUFF|              false|\n",
      "+-----------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"author\", \"title\", df.title.startswith(\"THE\")).show(5)\n",
    "df.select(\"author\", \"title\", df.title.endswith(\"NT\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [7] 'Substring' Operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, texts are extracted from the index numbers (1, 3), (3, 6) and (1, 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|title|\n",
      "+-----+\n",
      "|  Dea|\n",
      "|  Ste|\n",
      "|  Emi|\n",
      "|  Pat|\n",
      "|  Chu|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+\n",
      "| title|\n",
      "+------+\n",
      "|an R K|\n",
      "|epheni|\n",
      "|ily Gi|\n",
      "|tricia|\n",
      "|uck Pa|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+\n",
      "| title|\n",
      "+------+\n",
      "|Dean R|\n",
      "|Stephe|\n",
      "|Emily |\n",
      "|Patric|\n",
      "|Chuck |\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.author.substr(1, 3).alias(\"title\")).show(5)\n",
    "df.select(df.author.substr(3, 6).alias(\"title\")).show(5)\n",
    "df.select(df.author.substr(1, 6).alias(\"title\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [8] Adding Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: struct<$oid:string>, amazon_product_url: string, author: string, bestsellers_date: struct<$date:struct<$numberLong:string>>, description: string, price: struct<$numberDouble:string,$numberInt:string>, published_date: struct<$date:struct<$numberLong:string>>, publisher: string, rank: struct<$numberInt:string>, rank_last_week: struct<$numberInt:string>, title: string, weeks_on_list: struct<$numberInt:string>, new_column: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Lit() is required while we are creating columns with exact values.\n",
    "df = df.withColumn('new_column', F.lit('This is a new column'))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [9] Updating Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For updated operations of DataFrame API, withColumnRenamed() function is used with two parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulate column by adding 100\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "casesWithNewConfirmed = cases.withColumn(\"NewConfirmed\", 100 + F.col(\"confirmed\"))\n",
    "casesWithNewConfirmed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------------------------------------+\n",
      "|               title|CASE WHEN (NOT (title = ODD HOURS)) THEN 1 ELSE 0 END|\n",
      "+--------------------+-----------------------------------------------------+\n",
      "|           ODD HOURS|                                                    0|\n",
      "|            THE HOST|                                                    1|\n",
      "|LOVE THE ONE YOU'...|                                                    1|\n",
      "|           THE FRONT|                                                    1|\n",
      "|               SNUFF|                                                    1|\n",
      "|SUNDAYS AT TIFFANYâ€™S|                                                    1|\n",
      "|        PHANTOM PREY|                                                    1|\n",
      "|          SWINE NOT?|                                                    1|\n",
      "|     CARELESS IN RED|                                                    1|\n",
      "|     THE WHOLE TRUTH|                                                    1|\n",
      "+--------------------+-----------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show title and assign 0 or 1 to new column depending on title\n",
    "df.select(\"title\", when(df.title != 'ODD HOURS', 1).otherwise(0)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "\"requirement failed: The number of columns doesn't match.\\nOld column names (13): _id, amazon_product_url, author, bestsellers_date, description, price, published_date, publisher, rank, rank_last_week, title, weeks_on_list, new_column\\nNew column names (8): case_id, province, city, group, infection_case, confirmed, latitude, longitude\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Applications/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o142.toDF.\n: java.lang.IllegalArgumentException: requirement failed: The number of columns doesn't match.\nOld column names (13): _id, amazon_product_url, author, bestsellers_date, description, price, published_date, publisher, rank, rank_last_week, title, weeks_on_list, new_column\nNew column names (8): case_id, province, city, group, infection_case, confirmed, latitude, longitude\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.sql.Dataset.toDF(Dataset.scala:447)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-eda2ec76cc69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Change all column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m cases = df.toDF(*['case_id', 'province', 'city', 'group', 'infection_case', 'confirmed',\n\u001b[0;32m----> 6\u001b[0;31m        'latitude', 'longitude'])\n\u001b[0m",
      "\u001b[0;32m/Applications/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \"\"\"\n\u001b[0;32m-> 2063\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2064\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: \"requirement failed: The number of columns doesn't match.\\nOld column names (13): _id, amazon_product_url, author, bestsellers_date, description, price, published_date, publisher, rank, rank_last_week, title, weeks_on_list, new_column\\nNew column names (8): case_id, province, city, group, infection_case, confirmed, latitude, longitude\""
     ]
    }
   ],
   "source": [
    "#Change single column name\n",
    "cases = df.withColumnRenamed(\"infection_case\",\"infection_source\")\n",
    "\n",
    "#Change all column name\n",
    "cases = df.toDF(*['case_id', 'province', 'city', 'group', 'infection_case', 'confirmed',\n",
    "       'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change type of column\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "cases = cases.withColumn('confirmed', F.col('confirmed').cast(IntegerType()))\n",
    "cases = cases.withColumn('city', F.col('city').cast(StringType()))\n",
    "\n",
    "cases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 URL|           author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]|[[1212883200000]]|       Putnam| [4]|           [0]|           THE FRONT|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]|[[1212883200000]]|    Doubleday| [5]|           [0]|               SNUFF|          [1]|This is a new column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update column 'amazon_product_url' with 'URL'\n",
    "df = df.withColumnRenamed('amazon_product_url', 'URL')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [10] Removing Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of a column can be achieved in two ways: \\\n",
    "1. Adding the list of column names in the drop() function \n",
    "2. Specifying columns by pointing in the drop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop a list of columns\n",
    "drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "data = data.select([column for column in data.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 URL|           author| bestsellers_date|         description|   price|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]| [4]|           [0]|           THE FRONT|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]| [5]|           [0]|               SNUFF|          [1]|This is a new column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 URL|           author| bestsellers_date|         description|   price|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|    Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|     Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Patricia Cornwell|[[1211587200000]]|A Massachusetts s...|[22.95,]| [4]|           [0]|           THE FRONT|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Chuck Palahniuk|[[1211587200000]]|An aging porn que...|[24.95,]| [5]|           [0]|               SNUFF|          [1]|This is a new column|\n",
      "+--------------------+--------------------+-----------------+-----------------+--------------------+--------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "dataframe_remove = df.drop(\"publisher\", \"published_date\").show(5)\n",
    "\n",
    "# 2.\n",
    "dataframe_remove2 = df.drop(df.publisher).drop(df.published_date).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [11] 'GroupBy' Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              author|count|\n",
      "+--------------------+-----+\n",
      "|          James Frey|    2|\n",
      "|    Elin Hilderbrand|   58|\n",
      "|   Sharon Kay Penman|    2|\n",
      "|         Kate Jacobs|    3|\n",
      "|       Karen Robards|    6|\n",
      "|     Gary Shteyngart|    3|\n",
      "|         Lisa Genova|    7|\n",
      "|James Patterson a...|   30|\n",
      "|         Ruth Reichl|    3|\n",
      "|         JRR Tolkien|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by author, count the books of the authors in the groups\n",
    "\n",
    "df.groupBy(\"author\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 crimes\n",
    "from pyspark.sql.functions import col\n",
    "data.groupBy(\"Category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [12] 'Filter' Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter all the different infection_case in Daegu with more than 10 confirmed cases.\n",
    "cases.filter((cases.confirmed>10) & (cases.province=='Daegu')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
      "|                 _id|                 URL|         author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|   title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1212192000000]]|Aliens have taken...|[25.99,]|[[1213488000000]]|Little, Brown| [2]|           [2]|THE HOST|          [4]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1212796800000]]|Aliens have taken...|[25.99,]|[[1214092800000]]|Little, Brown| [2]|           [2]|THE HOST|          [5]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1213401600000]]|Aliens have taken...|[25.99,]|[[1214697600000]]|Little, Brown| [3]|           [2]|THE HOST|          [6]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1214006400000]]|Aliens have taken...|[25.99,]|[[1215302400000]]|Little, Brown| [3]|           [3]|THE HOST|          [7]|This is a new column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering entries of title\n",
    "# Only keeps records having value 'THE HOST'\n",
    "\n",
    "df.filter(df[\"title\"] == 'THE HOST').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [13] Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replacing null values\n",
    "# dataframe.na.fill()\n",
    "# dataFrame.fillna()\n",
    "# dataFrameNaFunctions.fill()\n",
    "\n",
    "# # Returning new dataframe restricting rows with null valuesdataframe.na.drop()\n",
    "# dataFrame.dropna()\n",
    "# dataFrameNaFunctions.drop()\n",
    "\n",
    "# # Return new dataframe replacing one value with another\n",
    "# dataframe.na.replace(5, 15)\n",
    "# dataFrame.replace()\n",
    "# dataFrameNaFunctions.replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [14] Running SQL Commnads In Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|                 _id|                 URL|         author| bestsellers_date|         description|   price|   published_date|    publisher|rank|rank_last_week|               title|weeks_on_list|          new_column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|  Dean R Koontz|[[1211587200000]]|Odd Thomas, who c...|  [, 27]|[[1212883200000]]|       Bantam| [1]|           [0]|           ODD HOURS|          [1]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|Stephenie Meyer|[[1211587200000]]|Aliens have taken...|[25.99,]|[[1212883200000]]|Little, Brown| [2]|           [1]|            THE HOST|          [3]|This is a new column|\n",
      "|[5b4aa4ead3089013...|http://www.amazon...|   Emily Giffin|[[1211587200000]]|A woman's happy m...|[24.95,]|[[1212883200000]]| St. Martin's| [3]|           [2]|LOVE THE ONE YOU'...|          [2]|This is a new column|\n",
      "+--------------------+--------------------+---------------+-----------------+--------------------+--------+-----------------+-------------+----+--------------+--------------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering a table\n",
    "df.registerTempTable(\"df\")\n",
    "\n",
    "sc.sql(\"select * from df\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use SQL commands with Spark\n",
    "cases.registerTempTable('cases_table')\n",
    "newDF = SQLContext.sql('select * from cases_table where confirmed > 100')\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15] Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Join 'Case' with 'Region' on Province and City column\n",
    "cases = cases.join(regions, ['province','city'],how='left')\n",
    "cases.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [16] Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark User Defined Function, confirmed is a column and cases is dataframe\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def casesHighLow(confirmed):\n",
    "    if confirmed < 50: \n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'high'\n",
    "    \n",
    "#convert to a UDF Function by passing in the function and return type of function\n",
    "casesHighLowUDF = F.udf(casesHighLow, StringType())\n",
    "CasesWithHighLow = cases.withColumn(\"HighLow\", casesHighLowUDF(\"confirmed\"))\n",
    "CasesWithHighLow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_iqr(data):\n",
    "    '''\n",
    "    This function returns the indices of outliers in data.\n",
    "    \n",
    "    INPUT:\n",
    "    data - list containing values\n",
    "    \n",
    "    OUTPUT:\n",
    "    Outlier indices    \n",
    "    '''\n",
    "    avg = np.mean(data)\n",
    "    lower_bound = avg - 2*np.std(data)\n",
    "    upper_bound = avg + 2*np.std(data)\n",
    "    return np.where((data > upper_bound) | (data < lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [17] Write & Save to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any data source type that is loaded to our code as data frames can easily be converted and saved into other types including .parquet and .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save and load Spark Data at Intermediate Step\n",
    "#When you work with Spark you will frequently run with memory and storage issues. While in some cases such issues might be resolved using techniques like broadcasting, salting or cache, sometimes just interrupting the workflow and saving and reloading the whole dataframe at a crucial step has helped me a lot. This helps spark to let go of a lot of memory that gets utilized for storing intermediate shuffle data and unused caches.\n",
    "\n",
    "df.write.parquet(\"data/df.parquet\")\n",
    "df.unpersist()\n",
    "spark.read.load(\"data/df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write & Save File in .parquet format\n",
    "dataframe.select(\"author\", \"title\", \"rank\", \"description\") \\\n",
    "        .write \\\n",
    "        .save(\"Rankings_Descriptions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write & Save File in .json format\n",
    "dataframe.select(\"author\", \"title\") \\\n",
    "        .write \\\n",
    "        .save(\"Authors_Titles.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [17] Write & Save to Files\n",
    "\n",
    "Any data source type that is loaded to our code as data frames can easily be converted and saved into other types including .parquet and .json\n",
    "\n",
    "# Write & Save File in Queretarouet format\n",
    "dataframe.select(\"author\", \"title\", \"rank\", \"description\") \\\n",
    "        .write \\\n",
    "        .save(\"Rankings_Descriptions.parquet\")\n",
    "\n",
    "# Write & Save File in .json format\n",
    "dataframe.select(\"author\", \"title\") \\\n",
    "        .write \\\n",
    "        .save(\"Authors_Titles.json\",format=\"json\")sc.sql(\"SELECT CASE WHEN description LIKE '%love%' THEN 'Love_Theme'             \n",
    "           WHEN description LIKE '%hate%' THEN 'Hate_Theme'            \n",
    "           WHEN description LIKE '%happy%' THEN 'Happiness_Theme' \\            \n",
    "           WHEN description LIKE '%anger%' THEN 'Anger_Theme' \\             \n",
    "           WHEN description LIKE '%horror%' THEN 'Horror_Theme' \\              \n",
    "           WHEN description LIKE '%death%' THEN 'Criminal_Theme' \\               \n",
    "           WHEN description LIKE '%detective%' THEN 'Mystery_Theme' ELSE 'Other_Themes' END Themes from df\").groupBy('Themes').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
