{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason for Spark over Pandas\n",
    "\n",
    "1) Pandas data frame is not distributed & Spark's DataFrame is distributed. -> Hence you won't get the benefit of parallel processing in Pandas DataFrame & speed of processing in Pandas DataFrame will be less for large amount of data.\n",
    "\n",
    "2) Spark DataFrame assures you fault tolerance (It's resilient) & pandas DataFrame does not assure it. -> Hence if your data processing got interrupted/failed in between processing then spark can regenerate the failed result set from lineage (from DAG) . Fault tolerance is not supported in Pandas. You need to implement your own framework to assure it.\n",
    "\n",
    "3) In my experience as a Data Engineer, I’ve found building data pipelines in Pandas often requires us to regularly increase resources to keep up with the increasing memory usage. In addition, we often see many runtime errors due to unexpected data types or nulls. As a result of using Spark with Scala instead, solutions feel more robust and easier to refactor and extend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "\n",
    "import pyspark # only run this after findspark.init()\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init('/Applications/spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.appName('Spark_Functions').master(\"local[*]\").getOrCreate()\n",
    "sparkContext=sc.sparkContext\n",
    "sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the below function to find more information about the #missing values\n",
    "def info_missing_table(df_pd):\n",
    "    \"\"\"Input pandas dataframe and Return columns with missing value and percentage\"\"\"\n",
    "    mis_val = df_pd.isnull().sum() #count total of null in each columns in dataframe\n",
    "#count percentage of null in each columns\n",
    "    mis_val_percent = 100 * df_pd.isnull().sum() / len(df_pd) \n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) \n",
    " #join to left (as column) between mis_val and mis_val_percent\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'}) \n",
    "#rename columns in table\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1) \n",
    "        \n",
    "    print (\"Your selected dataframe has \" + str(df_pd.shape[1]) + \" columns.\\n\"    #.shape[1] : just view total columns in dataframe  \n",
    "    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +              \n",
    "    \" columns that have missing values.\") #.shape[0] : just view total rows in dataframe\n",
    "    return mis_val_table_ren_columns\n",
    "missings = info_missing_table(df_pd)\n",
    "missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missings(spark_df):\n",
    "    null_counts = []        \n",
    "    for col in spark_df.dtypes:    \n",
    "        cname = col[0]     \n",
    "        ctype = col[1]      \n",
    "        nulls = spark_df.where( spark_df[cname].isNull()).count() #check count of null in column name\n",
    "        result = tuple([cname, nulls])  #new tuple, (column name, null count)\n",
    "        null_counts.append(result)      #put the new tuple in our result list\n",
    "    null_counts=[(x,y) for (x,y) in null_counts if y!=0]  #view just columns that have missing values\n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_counts = count_missings(new_df)\n",
    "miss_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical columns with missing values based on types\n",
    "\n",
    "list_cols_miss=[x[0] for x in miss_counts]\n",
    "df_miss= new_df.select(*list_cols_miss)\n",
    "\n",
    "# categorical columns\n",
    "catcolums_miss=[item[0] for item in df_miss.dtypes if item[1].startswith('string')]  #will select name of column with string data type\n",
    "print(\"cateogrical columns_miss:\", catcolums_miss)\n",
    "\n",
    "# numerical columns\n",
    "numcolumns_miss = [item[0] for item in df_miss.dtypes if item[1].startswith('int') | item[1].startswith('double')] #will select name of column with integer or double data type\n",
    "print(\"numerical columns_miss:\", numcolumns_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank, sum ,col, mean, round\n",
    "\n",
    "def impute_data(miss_counts):\n",
    "    list_cols_miss=[x[0] for x in miss_counts]\n",
    "    df_miss= new_df.select(*list_cols_miss)\n",
    "    # categorical columns\n",
    "    \n",
    "    #will select name of column with string data type\n",
    "    catcolums_miss=[item[0] for item in df_miss.dtypes if item[1].startswith('string')]\n",
    "    \n",
    "    # numerical columns\n",
    "    numcolumns_miss = [item[0] for item in df_miss.dtypes if item[1].startswith('int') | item[1].startswith('double')]\n",
    "    \n",
    "    #dropped_df\n",
    "    df_Nomiss=new_df.na.drop()\n",
    "    \n",
    "    #categorical operation\n",
    "    for x in catcolums_miss:                  \n",
    "    mode=df_Nomiss.groupBy(x).count().sort(col(\"count\").desc()).collect()[0][0] \n",
    "    print(x, mode) #print name of columns and it's most categories \n",
    "    new_df = new_df.na.fill({x:mode})\n",
    "    \n",
    "    #numerical operation\n",
    "    for i in numcolumns_miss:\n",
    "    meanvalue = new_df.select(round(mean(i))).collect()[0][0] \n",
    "    print(i, meanvalue) \n",
    "    new_df=new_df.na.fill({i:meanvalue})\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-------+-------+-------+--------+--------+--------+\n",
      "| ID|TYPE|CODE|eTYPE_A|eTYPE_B|eTYPE_C|eCODE_X1|eCODE_X2|eCODE_X3|\n",
      "+---+----+----+-------+-------+-------+--------+--------+--------+\n",
      "|  1|   A|  X1|      1|      0|      0|       1|       0|       0|\n",
      "|  2|   C|  X2|      0|      0|      1|       0|       1|       0|\n",
      "|  3|   B|  X3|      0|      1|      0|       0|       0|       1|\n",
      "|  2|   B|  X2|      0|      1|      0|       0|       1|       0|\n",
      "|  3|   C|  X2|      0|      0|      1|       0|       1|       0|\n",
      "|  1|   B|  X3|      0|      1|      0|       0|       0|       1|\n",
      "|  1|   B|  X1|      0|      1|      0|       1|       0|       0|\n",
      "|  1|   C|  X1|      0|      0|      1|       1|       0|       0|\n",
      "+---+----+----+-------+-------+-------+--------+--------+--------+\n",
      "\n",
      "+---+----+----+-------+-------+--------+--------+\n",
      "| ID|TYPE|CODE|eTYPE_B|eTYPE_C|eCODE_X2|eCODE_X3|\n",
      "+---+----+----+-------+-------+--------+--------+\n",
      "|  1|   A|  X1|      0|      0|       0|       0|\n",
      "|  2|   C|  X2|      0|      1|       1|       0|\n",
      "|  3|   B|  X3|      1|      0|       0|       1|\n",
      "|  2|   B|  X2|      1|      0|       1|       0|\n",
      "|  3|   C|  X2|      0|      1|       1|       0|\n",
      "|  1|   B|  X3|      1|      0|       0|       1|\n",
      "|  1|   B|  X1|      1|      0|       0|       0|\n",
      "|  1|   C|  X1|      0|      1|       0|       0|\n",
      "+---+----+----+-------+-------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eTYPE_A', 'eCODE_X1']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pandas One hot encoding\n",
    "#Best Practice\n",
    "#Still Needs to Drop One Value\n",
    "\n",
    "list_of_pivot_cols = ['TYPE','CODE']\n",
    "list_of_keys = ['ID','TYPE','CODE']\n",
    "\n",
    "Spark_DF = sparkContext.parallelize([(1,'A','X1'),\n",
    "                         (2,'B','X2'),\n",
    "                         (3,'B','X3'),\n",
    "                         (1,'B','X3'),\n",
    "                         (2,'C','X2'),\n",
    "                         (3,'C','X2'),\n",
    "                         (1,'C','X1'),\n",
    "                         (1,'B','X1')]).toDF(['ID','TYPE','CODE'])                         \n",
    "\n",
    "#Helper function to recursively join a list of dataframes\n",
    "#Can be simplified if you only need two columns\n",
    "\n",
    "class Spark_One_Hot_Encoding():\n",
    "    def __init__(self, list_of_keys, list_of_pivot_cols, Spark_DF):\n",
    "        pass\n",
    "    \n",
    "    def join_all(dfs, list_of_keys):\n",
    "        if len(dfs) > 1:\n",
    "            return dfs[0].join(join_all(dfs[1:], list_of_keys), on = list_of_keys, how = 'inner')\n",
    "        else:\n",
    "            return dfs[0]\n",
    "        \n",
    "    def encoded_df():\n",
    "        dfs = []\n",
    "        combined = []\n",
    "        \n",
    "        for pivot_col in pivot_cols:\n",
    "            pivotDF = Spark_DF.groupBy(keys).pivot(pivot_col).count()\n",
    "            new_names = pivotDF.columns[:len(keys)] +  [\"e{0}_{1}\".format(pivot_col, c) for c in pivotDF.columns[len(keys):]]\n",
    "            df = pivotDF.toDF(*new_names).fillna(0)\n",
    "            combined.append(df)\n",
    "            \n",
    "        new_df = join_all(combined, list_of_keys)\n",
    "        return new_df\n",
    "    \n",
    "    def encoded_drop1():\n",
    "        dfs = []\n",
    "        combined = []\n",
    "        drop_columns_list = []\n",
    "        \n",
    "        for pivot_col in pivot_cols:\n",
    "            pivotDF = Spark_DF.groupBy(keys).pivot(pivot_col).count()\n",
    "            new_names = pivotDF.columns[:len(keys)] +  [\"e{0}_{1}\".format(pivot_col, c) for c in pivotDF.columns[len(keys):]]\n",
    "            drop_columns_list.append(new_names[len(list_of_keys)])\n",
    "            df = pivotDF.toDF(*new_names).fillna(0)\n",
    "            combined.append(df)\n",
    "            \n",
    "        new_df = join_all(combined, list_of_keys)\n",
    "    \n",
    "        new_df_drop_1 = new_df.drop(*drop_columns_list)  \n",
    "        return new_df_drop_1, drop_columns_list\n",
    "        \n",
    "    \n",
    "Spark_One_Hot_Encoding(list_of_keys, list_of_pivot_cols, Spark_DF)\n",
    "Spark_One_Hot_Encoding.encoded_df().show()\n",
    "\n",
    "a, b = Spark_One_Hot_Encoding.encoded_drop1()\n",
    "a.show()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[ID: bigint, TYPE: string, CODE: string, eTYPE_A: bigint, eTYPE_B: bigint, eTYPE_C: bigint],\n",
       " DataFrame[ID: bigint, TYPE: string, CODE: string, eCODE_X1: bigint, eCODE_X2: bigint, eCODE_X3: bigint]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "| ID|TYPE|CODE|e_TYPE_A|e_TYPE_B|e_TYPE_C|e_CODE_X1|e_CODE_X2|e_CODE_X3|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "|  1|   A|  X1|       1|       0|       0|        1|        0|        0|\n",
      "|  2|   C|  X2|       0|       0|       1|        0|        1|        0|\n",
      "|  3|   B|  X3|       0|       1|       0|        0|        0|        1|\n",
      "|  2|   B|  X2|       0|       1|       0|        0|        1|        0|\n",
      "|  3|   C|  X2|       0|       0|       1|        0|        1|        0|\n",
      "|  1|   B|  X3|       0|       1|       0|        0|        0|        1|\n",
      "|  1|   B|  X1|       0|       1|       0|        1|        0|        0|\n",
      "|  1|   C|  X1|       0|       0|       1|        1|        0|        0|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_pivot_cols = ['TYPE','CODE']\n",
    "list_of_keys = ['ID','TYPE','CODE']\n",
    "\n",
    "Spark_DF = sparkContext.parallelize([(1,'A','X1'),\n",
    "                         (2,'B','X2'),\n",
    "                         (3,'B','X3'),\n",
    "                         (1,'B','X3'),\n",
    "                         (2,'C','X2'),\n",
    "                         (3,'C','X2'),\n",
    "                         (1,'C','X1'),\n",
    "                         (1,'B','X1')]).toDF(['ID','TYPE','CODE'])\n",
    "\n",
    "def join_all(dfs, list_of_keys):\n",
    "    if len(dfs) > 1:\n",
    "        return dfs[0].join(join_all(dfs[1:], list_of_keys), on = list_of_keys, how = 'inner')\n",
    "    else:\n",
    "        return dfs[0]\n",
    "dfs = []\n",
    "combined = []\n",
    "first_new_names = []\n",
    "        \n",
    "for pivot_col in pivot_cols:\n",
    "    pivotDF = Spark_DF.groupBy(keys).pivot(pivot_col).count()\n",
    "    new_names = pivotDF.columns[:len(keys)] +  [\"e_{0}_{1}\".format(pivot_col, c) for c in pivotDF.columns[len(keys):]]\n",
    "    first_new_names.append(new_names)\n",
    "    df = pivotDF.toDF(*new_names).fillna(0)\n",
    "    combined.append(df)\n",
    "    \n",
    "new_df = join_all(combined, list_of_keys)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-6768927772c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mweight_balance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_balance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Column Weights\n",
    "# adding the new column weights and fill it with ratios\n",
    "from pyspark.sql.functions import when\n",
    "ratio = 0.91\n",
    "def weight_balance(labels):\n",
    "    return when(labels == 1, ratio).otherwise(1*(1-ratio))\n",
    "new_df = new_df.withColumn('weights', weight_balance(col('label')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "PySpark has a great feature engineering handling, so that we do not need to do much for extracting features\n",
    "\n",
    "1. Apply StringIndexer() to assign indices to each category in our categorical columns\n",
    "2. Apply OneHotEncoderEstimator() to convert categorical columns to onehot encoded vectors\n",
    "3. Apply VectorAssembler() to create a feature vector from all categorical and numerical features and we call the final vector as “features”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the OneHotEncoderEstimator from MLlib in spark to convert #aech v=categorical feature into one-hot vectors\n",
    "# next, we use VectorAssembler to combine the resulted one-hot ector #and the rest of numerical features into a \n",
    "# single vector column. we append every step of the process in a #stages array\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "stages = []\n",
    "for categoricalCol in cat_cols:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "assemblerInputs = [c + \"classVec\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "cols = new_df.columns\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(new_df)\n",
    "new_df = pipelineModel.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedCols = ['features']+cols\n",
    "new_df = new_df.select(selectedCols)\n",
    "\n",
    "# New dataset after feature engineering:\n",
    "pd.DataFrame(new_df.take(5), columns=new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/Test Split\n",
    "# split the data into trainign and testin sets\n",
    "train, test = new_df.randomSplit([0.80, 0.20], seed = 42)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test SET ROC: 0.7193116947055981\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictions_LR = LR_model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test SET ROC: \" + str(evaluator.evaluate(predictions_LR, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_SET (Area Under ROC): 0.7323118190985889\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=15)\n",
    "GBT_Model = gbt.fit(train)\n",
    "gbt_predictions = GBT_Model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test SET ROC: 0.7323118190985882\n"
     ]
    }
   ],
   "source": [
    "print(\"Test SET ROC: \" + str(evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Gradient Boosting result then apply hyper-parameter tuning using grid search and after that we run cross validation to better improve the performance of GBT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbt.maxBins, [20, 30])\n",
    "             .addGrid(gbt.maxIter, [10, 15])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations.\n",
    "cvModel = cv.fit(train)\n",
    "gbt_cv_predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(gbt_cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=15)\n",
    "GBT_Model = gbt.fit(train)\n",
    "gbt_predictions = GBT_Model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test SET ROC: \" + str(evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbt.maxBins, [20, 30])\n",
    "             .addGrid(gbt.maxIter, [10, 15])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations.\n",
    "cvModel = cv.fit(train)\n",
    "gbt_cv_predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(gbt_cv_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
